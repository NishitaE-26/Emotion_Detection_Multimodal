{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rashmikr1203/project_try/blob/main/AudioCombine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Rashmi"
      ],
      "metadata": {
        "id": "j2j64bfbET_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 sessions, relu, sigmoid, with 10 features\n"
      ],
      "metadata": {
        "id": "vyY7s8jaWMnM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSZiEEV_WG6y"
      },
      "outputs": [],
      "source": [
        "#rashmi- #\n",
        "#session 4 sigmoid softmax\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def extract_features(file_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "        # Extracting features from librosa.effects and other suitable librosa functions\n",
        "        rms = librosa.feature.rms(y=y)[0]\n",
        "        zcr = librosa.feature.zero_crossing_rate(y=y)[0]\n",
        "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
        "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n",
        "        spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "        spectral_flatness = librosa.feature.spectral_flatness(y=y)\n",
        "        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
        "        chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)\n",
        "\n",
        "        # Additional features from librosa (these are not in librosa.effects)\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "        tonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n",
        "\n",
        "        # Combine all features into a single feature vector\n",
        "        features = np.hstack((\n",
        "            np.mean(rms), np.std(rms),\n",
        "            np.mean(zcr), np.std(zcr),\n",
        "            np.mean(spectral_centroid), np.std(spectral_centroid),\n",
        "            np.mean(spectral_bandwidth), np.std(spectral_bandwidth),\n",
        "            np.mean(spectral_contrast, axis=1), np.std(spectral_contrast, axis=1),\n",
        "            np.mean(spectral_flatness), np.std(spectral_flatness),\n",
        "            np.mean(spectral_rolloff), np.std(spectral_rolloff),\n",
        "            np.mean(chroma_cens, axis=1), np.std(chroma_cens, axis=1),\n",
        "            np.mean(mfccs, axis=1), np.std(mfccs, axis=1),\n",
        "            np.mean(tonnetz, axis=1), np.std(tonnetz, axis=1)\n",
        "        ))\n",
        "\n",
        "        return features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Define your dataset path\n",
        "dataset_path = '/content/drive/My Drive/session 1 2 3 4'\n",
        "\n",
        "# Prepare data containers\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# Emotion labels mapping\n",
        "emotion_labels = {\n",
        "    'happy': 0,\n",
        "    'sadness': 1,\n",
        "    'anger': 2,\n",
        "    'disgust': 3,\n",
        "    'fear': 4,\n",
        "    'neutral': 5,\n",
        "    'surprise': 6,\n",
        "    'sarcastic': 7,\n",
        "}\n",
        "\n",
        "# Load the dataset and extract features\n",
        "for emotion, label in emotion_labels.items():\n",
        "    emotion_path = os.path.join(dataset_path, emotion)\n",
        "    if not os.path.exists(emotion_path):\n",
        "        print(f\"Directory {emotion_path} does not exist!\")\n",
        "        continue\n",
        "\n",
        "    for file_name in os.listdir(emotion_path):\n",
        "        if file_name.endswith('.wav'):\n",
        "            file_path = os.path.join(emotion_path, file_name)\n",
        "            if not os.path.isfile(file_path):\n",
        "                print(f\"File {file_path} does not exist!\")\n",
        "                continue\n",
        "\n",
        "            features = extract_features(file_path)\n",
        "            if features is not None:\n",
        "                X.append(features)\n",
        "                y.append(label)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(f\"Extracted features for {len(X)} files.\")\n",
        "\n",
        "if X.size == 0:\n",
        "    print(\"No features were extracted. Exiting.\")\n",
        "else:\n",
        "    # Impute missing values using KNN\n",
        "    imputer = KNNImputer(n_neighbors=5)\n",
        "    X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define the Keras model with 5 layers\n",
        "    model = Sequential([\n",
        "        Dense(10, input_dim=X_train.shape[1], activation='relu'),\n",
        "        Dense(20, activation='relu'),\n",
        "        Dense(30, activation='relu'),\n",
        "        Dense(15, activation='relu'),\n",
        "        Dense(8, activation='relu'),\n",
        "        Dense(len(emotion_labels), activation='softmax')\n",
        "    ])\n",
        "\n",
        "# Compile the Keras model\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                        loss='sparse_categorical_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "\n",
        "    # Train the Keras model\n",
        "    model.fit(X_train, y_train,\n",
        "                    epochs=100,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    verbose=1)\n",
        "  # Evaluate the Keras model\n",
        "    _, keras_accuracy = keras_model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f'Keras Model Accuracy: {keras_accuracy*100:.2f}%')\n",
        "\n",
        "    # Define a model to extract embeddings\n",
        "    embedding_model = Model(inputs=model.input,\n",
        "                            outputs=model.layers[-2].output)\n",
        "\n",
        "    # Generate embeddings for train and test sets\n",
        "    X_train_embeddings = embedding_model.predict(X_train)\n",
        "    X_test_embeddings = embedding_model.predict(X_test)\n",
        "\n",
        "    # Define the Random Forest Classifier model\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "\n",
        "    # Train the Random Forest model on embeddings\n",
        "    rf_model.fit(X_train_embeddings, y_train)\n",
        "\n",
        "    # Evaluate the Random Forest model\n",
        "    y_pred = rf_model.predict(X_test_embeddings)\n",
        "    rf_accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'Random Forest Model Accuracy: {rf_accuracy*100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#rashmi\n",
        "#1185files, 16 features, keras NN input to random forest\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def extract_features(file_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "        # Extracting features from librosa.effects and other suitable librosa functions\n",
        "        rms = librosa.feature.rms(y=y)[0]\n",
        "        zcr = librosa.feature.zero_crossing_rate(y=y)[0]\n",
        "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
        "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n",
        "        spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "        spectral_flatness = librosa.feature.spectral_flatness(y=y)\n",
        "        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
        "        chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)\n",
        "        spectral_flux = np.mean(librosa.onset.onset_strength(y=y, sr=sr)) # Spectral flux\n",
        "        pitches, magnitudes = librosa.core.piptrack(y=y, sr=sr)\n",
        "        pitch = np.mean(pitches[pitches > 0]) # Pitch\n",
        "\n",
        "        # Additional features from librosa (these are not in librosa.effects)\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "        tonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n",
        "\n",
        "        # Simplified jitter and shimmer calculation\n",
        "        frame_length = int(0.025 * sr)\n",
        "        hop_length = int(0.01 * sr)\n",
        "        pitches, magnitudes = librosa.core.piptrack(y=y, sr=sr, n_fft=frame_length, hop_length=hop_length)\n",
        "        valid_pitches = pitches[pitches > 0]\n",
        "        jitter = np.mean(np.abs(np.diff(valid_pitches))) / np.mean(valid_pitches)\n",
        "        shimmer = np.std(valid_pitches) / np.mean(valid_pitches)\n",
        "\n",
        "        # Extract Harmonics-to-Noise Ratio (HNR)\n",
        "        hnr = librosa.effects.harmonic(y)\n",
        "        mean_hnr = np.mean(hnr)\n",
        "        std_hnr = np.std(hnr)\n",
        "\n",
        "        # Extract Linear Predictive Coding (LPC)\n",
        "        lpc = librosa.lpc(y, order=10)\n",
        "        mean_lpc = np.mean(lpc)\n",
        "        std_lpc = np.std(lpc)\n",
        "\n",
        "        # Estimate speech rate and rhythm (number of onsets)\n",
        "        onsets = librosa.onset.onset_detect(y=y, sr=sr)\n",
        "        speech_rate = len(onsets) / (len(y) / sr)\n",
        "        rhythm = np.std(onsets)\n",
        "\n",
        "        # Combine all features into a single feature vector\n",
        "        features = np.hstack((\n",
        "            np.mean(rms), np.std(rms),\n",
        "            np.mean(zcr), np.std(zcr),\n",
        "            np.mean(spectral_centroid), np.std(spectral_centroid),\n",
        "            np.mean(spectral_bandwidth), np.std(spectral_bandwidth),\n",
        "            np.mean(spectral_contrast, axis=1), np.std(spectral_contrast, axis=1),\n",
        "            np.mean(spectral_flatness), np.std(spectral_flatness),\n",
        "            np.mean(spectral_rolloff), np.std(spectral_rolloff),\n",
        "            np.mean(chroma_cens, axis=1), np.std(chroma_cens, axis=1),\n",
        "            np.mean(mfccs, axis=1), np.std(mfccs, axis=1),\n",
        "            np.mean(tonnetz, axis=1), np.std(tonnetz, axis=1),\n",
        "            spectral_flux, pitch,\n",
        "            jitter, shimmer,\n",
        "            mean_hnr, std_hnr,\n",
        "            mean_lpc, std_lpc,\n",
        "            speech_rate, rhythm\n",
        "        ))\n",
        "\n",
        "        return features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Define your dataset path\n",
        "dataset_path = '/content/drive/My Drive/session 1 2 3 4 5'\n",
        "\n",
        "# Prepare data containers\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# Emotion labels mapping\n",
        "emotion_labels = {\n",
        "    'happy': 0,\n",
        "    'sadness': 1,\n",
        "    'anger': 2,\n",
        "    'disgust': 3,\n",
        "    'fear': 4,\n",
        "    'neutral': 5,\n",
        "    'surprise': 6,\n",
        "    'sarcastic': 7,\n",
        "}\n",
        "\n",
        "# Load the dataset and extract features\n",
        "for emotion, label in emotion_labels.items():\n",
        "    emotion_path = os.path.join(dataset_path, emotion)\n",
        "    if not os.path.exists(emotion_path):\n",
        "        print(f\"Directory {emotion_path} does not exist!\")\n",
        "        continue\n",
        "\n",
        "    for file_name in os.listdir(emotion_path):\n",
        "        if file_name.endswith('.wav'):\n",
        "            file_path = os.path.join(emotion_path, file_name)\n",
        "            if not os.path.isfile(file_path):\n",
        "                print(f\"File {file_path} does not exist!\")\n",
        "                continue\n",
        "\n",
        "            features = extract_features(file_path)\n",
        "            if features is not None:\n",
        "                X.append(features)\n",
        "                y.append(label)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(f\"Extracted features for {len(X)} files.\")\n",
        "\n",
        "if X.size == 0:\n",
        "    print(\"No features were extracted. Exiting.\")\n",
        "else:\n",
        "    # Impute missing values using KNN\n",
        "    imputer = KNNImputer(n_neighbors=5)\n",
        "    X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define the Keras model with 5 layers\n",
        "    keras_model = Sequential([\n",
        "        Dense(10, input_dim=X_train.shape[1], activation='relu'),\n",
        "        Dense(20, activation='relu'),\n",
        "        Dense(30, activation='relu'),\n",
        "        Dense(15, activation='relu'),\n",
        "        Dense(8, activation='relu'),\n",
        "        Dense(len(emotion_labels), activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the Keras model\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    keras_model.compile(optimizer=optimizer,\n",
        "                        loss='sparse_categorical_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "\n",
        "    # Train the Keras model\n",
        "    keras_model.fit(X_train, y_train,\n",
        "                    epochs=100,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    verbose=1)\n",
        "\n",
        "     # Evaluate the Keras model\n",
        "    _, keras_accuracy = keras_model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f'Keras Model Accuracy: {keras_accuracy*100:.2f}%')\n",
        "\n",
        "    # Define a model to extract embeddings\n",
        "    embedding_model = Model(inputs=keras_model.input,\n",
        "                            outputs=keras_model.layers[-2].output)\n",
        "\n",
        "    # Generate embeddings for train and test sets\n",
        "    X_train_embeddings = embedding_model.predict(X_train)\n",
        "    X_test_embeddings = embedding_model.predict(X_test)\n",
        "\n",
        "    # Define the Random Forest Classifier model\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "\n",
        "    # Train the Random Forest model on embeddings\n",
        "    rf_model.fit(X_train_embeddings, y_train)\n",
        "\n",
        "    # Evaluate the Random Forest model\n",
        "    y_pred = rf_model.predict(X_test_embeddings)\n",
        "    rf_accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'Random Forest Model Accuracy: {rf_accuracy*100:.2f}%')\n"
      ],
      "metadata": {
        "id": "iWR7D9fnZtAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Nishita"
      ],
      "metadata": {
        "id": "6YL2cQx0EYD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15 features, 5 sessions, keras model and random forest with hyperparameter tuning using GridSearchCV"
      ],
      "metadata": {
        "id": "AG601e4NEHLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import parselmouth\n",
        "from parselmouth.praat import call\n",
        "\n",
        "# EXTRACTING FEATURES\n",
        "\n",
        "def extract_features(file_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "        # MFCCs\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "\n",
        "        # Chroma\n",
        "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "\n",
        "        # Spectral Contrast\n",
        "        contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "\n",
        "        # RMS (Root Mean Square) Energy\n",
        "        rms = librosa.feature.rms(y=y)\n",
        "\n",
        "        # Zero Crossing Rate\n",
        "        zcr = librosa.feature.zero_crossing_rate(y)\n",
        "\n",
        "        # Spectral Centroid\n",
        "        centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "\n",
        "        # Spectral Bandwidth\n",
        "        bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "\n",
        "        # Spectral Roll-off\n",
        "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "\n",
        "        # Spectral Flux\n",
        "        spectral_flux = np.mean(librosa.onset.onset_strength(y=y, sr=sr))\n",
        "\n",
        "        # Pitch (using the harmonic-percussive source separation)\n",
        "        pitches, magnitudes = librosa.core.piptrack(y=y, sr=sr)\n",
        "        pitch = []\n",
        "        for t in range(pitches.shape[1]):\n",
        "            index = magnitudes[:, t].argmax()\n",
        "            pitch.append(pitches[index, t])\n",
        "        pitch = np.array(pitch)\n",
        "        pitch = pitch[pitch > 0]  # Remove zero values\n",
        "\n",
        "        # Intensity (Using RMS Energy)\n",
        "        intensity = np.mean(rms)\n",
        "\n",
        "        # Load with parselmouth for advanced features\n",
        "        snd = parselmouth.Sound(file_path)\n",
        "\n",
        "        # Formants\n",
        "        formant1 = formant2 = jitter = hnr = np.nan\n",
        "        try:\n",
        "            formant_burg = snd.to_formant_burg()\n",
        "            formant1 = call(formant_burg, \"Get mean\", 1, 0, 0, \"Hertz\")\n",
        "            formant2 = call(formant_burg, \"Get mean\", 2, 0, 0, \"Hertz\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting formants: {e}\")\n",
        "\n",
        "        # Jitter\n",
        "        try:\n",
        "            point_process = call(snd, \"To PointProcess (periodic, cc)\", 75, 500)\n",
        "            jitter = call(point_process, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting jitter: {e}\")\n",
        "\n",
        "        # Harmonics to Noise Ratio\n",
        "        try:\n",
        "            harmonicity = call(snd, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
        "            hnr = call(harmonicity, \"Get mean\", 0, 0)\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting HNR: {e}\")\n",
        "\n",
        "        # Assemble the feature vector\n",
        "        features = np.hstack((\n",
        "            np.mean(mfccs, axis=1), np.std(mfccs, axis=1),\n",
        "            np.mean(chroma, axis=1), np.std(chroma, axis=1),\n",
        "            np.mean(contrast, axis=1), np.std(contrast, axis=1),\n",
        "            np.mean(rms), np.std(rms),\n",
        "            np.mean(zcr), np.std(zcr),\n",
        "            np.mean(centroid), np.std(centroid),\n",
        "            np.mean(bandwidth), np.std(bandwidth),\n",
        "            np.mean(rolloff), np.std(rolloff),\n",
        "            np.mean(pitch), np.std(pitch),\n",
        "            intensity,\n",
        "            spectral_flux,\n",
        "            formant1, formant2,\n",
        "            jitter,\n",
        "            hnr\n",
        "        ))\n",
        "\n",
        "        return features\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Define your dataset path\n",
        "dataset_path = '/content/drive/MyDrive/session 1 2 3 4 5'\n",
        "\n",
        "# Prepare data containers\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# Emotion labels mapping\n",
        "emotion_labels = {\n",
        "    'anger': 0,\n",
        "    'disgust': 1,\n",
        "    'fear': 2,\n",
        "    'happy': 3,\n",
        "    'neutral': 4,\n",
        "    'sadness': 5,\n",
        "    'sarcastic': 6,\n",
        "    'surprise': 7\n",
        "}\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "for emotion, label in emotion_labels.items():\n",
        "    emotion_path = os.path.join(dataset_path, emotion)\n",
        "    if not os.path.exists(emotion_path):\n",
        "        continue\n",
        "    for file_name in os.listdir(emotion_path):\n",
        "        file_path = os.path.join(emotion_path, file_name)\n",
        "        features = extract_features(file_path)\n",
        "        if features is not None:\n",
        "            X.append(features)\n",
        "            y.append(label)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(f\"Extracted features for {len(X)} files.\")"
      ],
      "metadata": {
        "id": "H8nabpovWv9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras model"
      ],
      "metadata": {
        "id": "9yZjFt8NErYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize features\n",
        "# Impute missing values using KNN\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imputed)\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define the Keras model\n",
        "model = Sequential([\n",
        "        Dense(10, input_dim=X_train.shape[1], activation='relu'),\n",
        "        Dense(20, activation='relu'),\n",
        "        Dense(30, activation='relu'),\n",
        "        Dense(15, activation='relu'),\n",
        "        Dense(8, activation='relu'),\n",
        "        Dense(len(emotion_labels), activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "history = model.fit(X_train, y_train,\n",
        "                        epochs=100,\n",
        "                        batch_size=32,\n",
        "                        validation_data=(X_test, y_test),\n",
        "                        verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "_, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m7Mz9gMdEngS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running keras model such that its output becomes input for random forest"
      ],
      "metadata": {
        "id": "sxbwMS3MEwYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize features\n",
        "# Impute missing values using KNN\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imputed)\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Keras model with 5 layers\n",
        "model = Sequential([\n",
        "        Dense(10, input_dim=X_train.shape[1], activation='relu'),\n",
        "        Dense(20, activation='relu'),\n",
        "        Dense(30, activation='relu'),\n",
        "        Dense(15, activation='relu'),\n",
        "        Dense(8, activation='relu'),\n",
        "        Dense(len(emotion_labels), activation='softmax')\n",
        "    ])\n",
        "\n",
        "# Compile the Keras model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer,\n",
        "                        loss='sparse_categorical_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "\n",
        "    # Train the Keras model\n",
        "model.fit(X_train, y_train,\n",
        "                    epochs=100,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    verbose=1)\n",
        "\n",
        "    # Define a model to extract embeddings\n",
        "embedding_model = Model(inputs=model.input,\n",
        "                            outputs=model.layers[-2].output)\n",
        "\n",
        "    # Generate embeddings for train and test sets\n",
        "X_train_embeddings = embedding_model.predict(X_train)\n",
        "X_test_embeddings = embedding_model.predict(X_test)\n",
        "\n",
        "    # Define the Random Forest Classifier model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "\n",
        "    # Train the Random Forest model on embeddings\n",
        "rf_model.fit(X_train_embeddings, y_train)\n",
        "\n",
        "    # Evaluate the Random Forest model\n",
        "y_pred = rf_model.predict(X_test_embeddings)\n",
        "rf_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Random Forest Model Accuracy: {rf_accuracy*100:.2f}%')"
      ],
      "metadata": {
        "id": "_e_wH20zEtGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Arundhati"
      ],
      "metadata": {
        "id": "r-GSWstIEbsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 16 features, 5 sessions, Random forest and keras models separately along with making into a dataframe"
      ],
      "metadata": {
        "id": "2Tvu12XfX9G5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#arundhati\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def extract_features(file_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "        # Extracting features from librosa.effects and other suitable librosa functions\n",
        "        rms = librosa.feature.rms(y=y)[0]\n",
        "        zcr = librosa.feature.zero_crossing_rate(y=y)[0]\n",
        "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
        "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n",
        "        spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "        spectral_flatness = librosa.feature.spectral_flatness(y=y)\n",
        "        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
        "        chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "        tonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n",
        "        spectral_flux = librosa.onset.onset_strength(y=y, sr=sr)\n",
        "\n",
        "        # Pitch extraction\n",
        "        pitches, magnitudes = librosa.core.piptrack(y=y, sr=sr)\n",
        "        pitch = [pitches[magnitudes[:, i].argmax(), i] for i in range(magnitudes.shape[1])]\n",
        "        pitch = np.array(pitch)\n",
        "\n",
        "        # Placeholder for jitter and shimmer (implement appropriate methods)\n",
        "        jitter = np.std(pitch)  # This is a placeholder for jitter calculation\n",
        "        shimmer = np.std(rms)   # This is a placeholder for shimmer calculation\n",
        "        hnr = librosa.effects.harmonic(y)  # Placeholder for HNR\n",
        "\n",
        "        # Formants\n",
        "        lpc_coeffs = librosa.lpc(y, order=2)\n",
        "\n",
        "        # Speech rate and rhythm (placeholders, actual extraction requires more complex algorithms)\n",
        "        speech_rate = len(librosa.effects.split(y, top_db=20)) / (len(y) / sr)\n",
        "        rhythm = np.std(librosa.beat.beat_track(y=y, sr=sr)[1])  # Placeholder for rhythm\n",
        "\n",
        "        # Combine all features into a single feature vector\n",
        "        features = np.hstack((\n",
        "            np.mean(rms), np.std(rms),\n",
        "            np.mean(zcr), np.std(zcr),\n",
        "            np.mean(spectral_centroid), np.std(spectral_centroid),\n",
        "            np.mean(spectral_bandwidth), np.std(spectral_bandwidth),\n",
        "            np.mean(spectral_contrast, axis=1), np.std(spectral_contrast, axis=1),\n",
        "            np.mean(spectral_flatness), np.std(spectral_flatness),\n",
        "            np.mean(spectral_rolloff), np.std(spectral_rolloff),\n",
        "            np.mean(chroma_cens, axis=1), np.std(chroma_cens, axis=1),\n",
        "            np.mean(mfccs, axis=1), np.std(mfccs, axis=1),\n",
        "            np.mean(tonnetz, axis=1), np.std(tonnetz, axis=1),\n",
        "            np.mean(spectral_flux), np.std(spectral_flux),\n",
        "            np.mean(pitch), np.std(pitch),\n",
        "            jitter, shimmer,\n",
        "            np.mean(hnr), np.std(hnr),\n",
        "            np.mean(lpc_coeffs), np.std(lpc_coeffs),\n",
        "            speech_rate, rhythm\n",
        "        ))\n",
        "\n",
        "        return features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Define your dataset path\n",
        "dataset_path = '/content/drive/MyDrive/session 1 2 3 4 5'\n",
        "\n",
        "# Prepare data containers\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# Emotion labels mapping\n",
        "emotion_labels = {\n",
        "    'happy': 0,\n",
        "    'sadness': 1,\n",
        "    'anger': 2,\n",
        "    'disgust': 3,\n",
        "    'fear': 4,\n",
        "    'neutral': 5,\n",
        "    'surprise': 6,\n",
        "    'sarcastic': 7,\n",
        "}\n",
        "\n",
        "# Load the dataset and extract features\n",
        "for emotion, label in emotion_labels.items():\n",
        "    emotion_path = os.path.join(dataset_path, emotion)\n",
        "    if not os.path.exists(emotion_path):\n",
        "        print(f\"Directory {emotion_path} does not exist!\")\n",
        "        continue\n",
        "\n",
        "    for file_name in os.listdir(emotion_path):\n",
        "        if file_name.endswith('.wav'):\n",
        "            file_path = os.path.join(emotion_path, file_name)\n",
        "            if not os.path.isfile(file_path):\n",
        "                print(f\"File {file_path} does not exist!\")\n",
        "                continue\n",
        "\n",
        "            features = extract_features(file_path)\n",
        "            if features is not None:\n",
        "                X.append(features)\n",
        "                y.append(label)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Create a DataFrame with feature columns\n",
        "columns = [\n",
        "    'mean_rms', 'std_rms',\n",
        "    'mean_zcr', 'std_zcr',\n",
        "    'mean_spectral_centroid', 'std_spectral_centroid',\n",
        "    'mean_spectral_bandwidth', 'std_spectral_bandwidth',\n",
        "    'mean_spectral_contrast_0', 'mean_spectral_contrast_1', 'mean_spectral_contrast_2',\n",
        "    'mean_spectral_contrast_3', 'mean_spectral_contrast_4', 'mean_spectral_contrast_5', 'mean_spectral_contrast_6',\n",
        "    'std_spectral_contrast_0', 'std_spectral_contrast_1', 'std_spectral_contrast_2',\n",
        "    'std_spectral_contrast_3', 'std_spectral_contrast_4', 'std_spectral_contrast_5', 'std_spectral_contrast_6',\n",
        "    'mean_spectral_flatness', 'std_spectral_flatness',\n",
        "    'mean_spectral_rolloff', 'std_spectral_rolloff',\n",
        "    'mean_chroma_cens_0', 'mean_chroma_cens_1', 'mean_chroma_cens_2',\n",
        "    'mean_chroma_cens_3', 'mean_chroma_cens_4', 'mean_chroma_cens_5',\n",
        "    'mean_chroma_cens_6', 'mean_chroma_cens_7', 'mean_chroma_cens_8', 'mean_chroma_cens_9', 'mean_chroma_cens_10',\n",
        "    'mean_chroma_cens_11',\n",
        "    'std_chroma_cens_0', 'std_chroma_cens_1', 'std_chroma_cens_2',\n",
        "    'std_chroma_cens_3', 'std_chroma_cens_4', 'std_chroma_cens_5',\n",
        "    'std_chroma_cens_6', 'std_chroma_cens_7', 'std_chroma_cens_8', 'std_chroma_cens_9', 'std_chroma_cens_10',\n",
        "    'std_chroma_cens_11',\n",
        "    'mean_mfcc_0', 'mean_mfcc_1', 'mean_mfcc_2', 'mean_mfcc_3', 'mean_mfcc_4',\n",
        "    'mean_mfcc_5', 'mean_mfcc_6', 'mean_mfcc_7', 'mean_mfcc_8', 'mean_mfcc_9',\n",
        "    'mean_mfcc_10', 'mean_mfcc_11', 'mean_mfcc_12',\n",
        "    'std_mfcc_0', 'std_mfcc_1', 'std_mfcc_2', 'std_mfcc_3', 'std_mfcc_4',\n",
        "    'std_mfcc_5', 'std_mfcc_6', 'std_mfcc_7', 'std_mfcc_8', 'std_mfcc_9',\n",
        "    'std_mfcc_10', 'std_mfcc_11', 'std_mfcc_12',\n",
        "    'mean_tonnetz_0', 'mean_tonnetz_1', 'mean_tonnetz_2',\n",
        "    'mean_tonnetz_3', 'mean_tonnetz_4', 'mean_tonnetz_5',\n",
        "    'std_tonnetz_0', 'std_tonnetz_1', 'std_tonnetz_2',\n",
        "    'std_tonnetz_3', 'std_tonnetz_4', 'std_tonnetz_5',\n",
        "    'mean_spectral_flux', 'std_spectral_flux',\n",
        "    'mean_pitch', 'std_pitch',\n",
        "    'jitter', 'shimmer',\n",
        "    'mean_hnr', 'std_hnr',\n",
        "    'mean_lpc', 'std_lpc',\n",
        "    'speech_rate', 'rhythm'\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(X, columns=columns)\n",
        "df['label'] = y\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "# Preprocessing\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=0.95)  # Preserve 95% of variance\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Keras model\n",
        "model = Sequential([\n",
        "    Dense(10, input_dim=X_train.shape[1], activation='relu'),\n",
        "    Dense(20, activation='relu'),\n",
        "    Dense(30, activation='relu'),\n",
        "    Dense(15, activation='relu'),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(len(emotion_labels), activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=100,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "_, nn_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Neural Network Accuracy: {nn_accuracy*100:.2f}%')\n",
        "\n",
        "# Random Forest\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "print(f'Random Forest Accuracy: {rf_accuracy*100:.2f}%')\n",
        "\n"
      ],
      "metadata": {
        "id": "rEvvOpTYWwKy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}